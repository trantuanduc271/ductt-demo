[
  {
    "template_name": "WORD_COUNT",
    "description": "A simple batch job to count word occurrences in a text file.",
    "template_path": "v1/",
    "template_gcs_path": "gs://dataflow-templates/latest/Word_Count",
    "type": "CLASSIC",
    "params": {
      "required": [
        "inputFile",
        "output"
      ],
      "optional": []
    }
  },
  {
    "template_name": "PubSub_Avro_to_BigQuery",
    "description": "A streaming job to read Avro-encoded messages from a Pub/Sub subscription and write them to a BigQuery table.",
    "template_path": "v2/pubsub-binary-to-bigquery",
    "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/PubSub_Avro_to_BigQuery",
    "type": "FLEX",
    "params": {
      "required": [
        "schemaPath",
        "inputSubscription",
        "outputTableSpec",
        "outputTopic"
      ],
      "optional": [
        "useStorageWriteApiAtLeastOnce",
        "writeDisposition",
        "createDisposition",
        "useStorageWriteApi",
        "numStorageWriteApiStreams",
        "storageWriteApiTriggeringFrequencySec"
      ]
    }
  },
  {
  "template_name": "Cloud_PubSub_to_Avro_Flex",
  "description": "The Pub/Sub to Avro files on Cloud Storage template is a streaming pipeline that reads data from a Pub/Sub topic and writes Avro files into the specified Cloud Storage bucket.",
  "template_path": "v2/googlecloud-to-googlecloud",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Cloud_PubSub_to_Avro_Flex",
  "type": "FLEX",
  "params": {
    "required": [
      "outputDirectory",
      "avroTempDirectory"
    ],
    "optional": [
      "inputSubscription",
      "inputTopic",
      "outputFilenamePrefix",
      "outputFilenameSuffix",
      "outputShardTemplate",
      "numShards",
      "windowDuration",
      "yearPattern",
      "monthPattern",
      "dayPattern",
      "hourPattern",
      "minutePattern"
    ]
  }
},
  {
    "template_name": "MongoDB_to_BigQuery_CDC",
    "description": "The MongoDB to BigQuery template is a streaming pipeline It works together with MongoDB change stream. The pipeline MongoDbToBigQueryCdc read the json pushed to Pub/Sub via MongoDB change stream and writes to BigQuery based on user input. Currently, this pipeline supports two types of userOptions. First is FLATTEN where the documents are Flattened to first level. Second is NONE where the documents are stored as a json string into BigQuery.",
    "template_path": "v2/mongodb-to-googlecloud",
    "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/MongoDB_to_BigQuery_CDC",
    "type": "FLEX",
    "params": {
      "required": [
        "mongoDbUri",
        "database",
        "collection",
        "inputTopic",
        "outputTableSpec",
        "userOption"
      ],
      "optional": []
    }
  },
  {
    "template_name": "MongoDB_to_BigQuery",
    "description": "The MongoDbToBigQuery pipeline Reads the data from MongoDb collection, Writes the data to BigQuery. The MongoDB to BigQuery template is a batch pipeline that reads document from MongoDB and writes them to BigQuery as specified in userOption. Currently, this pipeline supports two types of userOptions. First is FLATTEN where the documents are Flattened to first level. Second is NONE where the documents are stored as a json string into BigQuery.",
    "template_path": "v2/mongodb-to-googlecloud",
    "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/MongoDB_to_BigQuery",
    "type": "FLEX",
    "params": {
      "required": [
        "mongoDbUri",
        "database",
        "collection",
        "outputTableSpec",
        "userOption"
      ],
      "optional": []
    }
  },
  
  {
  "template_name": "PostgreSQL_to_BigQuery",
  "description": "The PostgreSQL to BigQuery template is a batch pipeline that copies data from a PostgreSQL table into an existing BigQuery table. This pipeline uses JDBC to connect to PostgreSQL. For an extra layer of protection, you can also pass in a Cloud KMS key along with Base64-encoded username, password, and connection string parameters encrypted with the Cloud KMS key.",
  "template_path": "v2/postgresql-to-googlecloud",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/PostgreSQL_to_BigQuery",
  "type": "FLEX",
  "params": {
    "required": [
      "connectionURL",
      "outputTable",
      "bigQueryLoadingTemporaryDirectory"
    ],
    "optional": [
      "connectionProperties",
      "username",
      "password",
      "query",
      "KMSEncryptionKey",
      "useColumnAlias",
      "isTruncate",
      "partitionColumn",
      "partitionColumnType",
      "table",
      "numPartitions",
      "lowerBound",
      "upperBound",
      "fetchSize",
      "createDisposition",
      "bigQuerySchemaPath",
      "outputDeadletterTable",
      "disabledAlgorithms",
      "extraFilesToStage",
      "useStorageWriteApi",
      "useStorageWriteApiAtLeastOnce"
    ]
  }
},
  {
    "template_name": "Bigtable_Change_Streams_to_HBase",
    "description": "A streaming pipeline that replicates Bigtable change stream mutations to HBase.",
    "template_path": "v2/bigtable-changestreams-to-hbase",
    "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Bigtable_Change_Streams_to_HBase",
    "type": "FLEX",
    "params": {
      "required": [
        "hbaseRootDir",
        "hbaseZookeeperQuorumHost",
        "bigtableChangeStreamAppProfile",
        "bigtableReadInstanceId",
        "bigtableReadTableId"
      ],
      "optional": [
        "bidirectionalReplicationEnabled",
        "cbtQualifier",
        "dryRunEnabled",
        "filterGCMutations",
        "hbaseQualifier",
        "hbaseZookeeperQuorumPort",
        "bigtableChangeStreamMetadataInstanceId",
        "bigtableChangeStreamMetadataTableTableId",
        "bigtableChangeStreamCharset",
        "bigtableChangeStreamStartTimestamp",
        "bigtableChangeStreamIgnoreColumnFamilies",
        "bigtableChangeStreamIgnoreColumns",
        "bigtableChangeStreamName",
        "bigtableChangeStreamResume",
        "bigtableReadChangeStreamTimeoutMs",
        "bigtableReadProjectId",
        "bigtableReadAppProfile",
        "bigtableRpcAttemptTimeoutMs",
        "bigtableRpcTimeoutMs",
        "bigtableAdditionalRetryCodes"
      ]
    }
  },
  {
    "template_name": "Bigtable_Change_Streams_to_Vector_Search",
    "description": "Streaming pipeline. Streams Bigtable data change records and writes them into Vertex AI Vector Search using Dataflow Runner V2.",
    "template_path": "v2/googlecloud-to-googlecloud",
    "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Bigtable_Change_Streams_to_Vector_Search",
    "type": "FLEX",
    "params": {
      "required": [
        "embeddingColumn",
        "embeddingByteSize",
        "vectorSearchIndex",
        "bigtableChangeStreamAppProfile",
        "bigtableReadInstanceId",
        "bigtableReadTableId"
      ],
      "optional": [
        "bigtableMetadataTableTableId",
        "crowdingTagColumn",
        "allowRestrictsMappings",
        "denyRestrictsMappings",
        "intNumericRestrictsMappings",
        "floatNumericRestrictsMappings",
        "doubleNumericRestrictsMappings",
        "upsertMaxBatchSize",
        "upsertMaxBufferDuration",
        "deleteMaxBatchSize",
        "deleteMaxBufferDuration",
        "dlqDirectory",
        "bigtableChangeStreamMetadataInstanceId",
        "bigtableChangeStreamMetadataTableTableId",
        "bigtableChangeStreamCharset",
        "bigtableChangeStreamStartTimestamp",
        "bigtableChangeStreamIgnoreColumnFamilies",
        "bigtableChangeStreamIgnoreColumns",
        "bigtableChangeStreamName",
        "bigtableChangeStreamResume",
        "bigtableReadChangeStreamTimeoutMs",
        "bigtableReadProjectId"
      ]
    }
  },
  {
    "template_name": "Bigtable_Change_Streams_to_BigQuery",
    "description": "Streaming pipeline. Streams Bigtable data change records and writes them into BigQuery using Dataflow Runner V2.",
    "template_path": "v2/googlecloud-to-googlecloud",
    "template_gcs_path": "gs://dataflow-templates/latest/flex/Bigtable_Change_Streams_to_BigQuery",
    "type": "FLEX",
    "params": {
      "required": [
        "bigQueryDataset",
        "bigtableChangeStreamAppProfile",
        "bigtableReadInstanceId",
        "bigtableReadTableId"
      ],
      "optional": [
        "writeRowkeyAsBytes",
        "writeValuesAsBytes",
        "writeNumericTimestamps",
        "bigQueryProjectId",
        "bigQueryChangelogTableName",
        "bigQueryChangelogTablePartitionGranularity",
        "bigQueryChangelogTablePartitionExpirationMs",
        "bigQueryChangelogTableFieldsToIgnore",
        "dlqDirectory",
        "bigtableChangeStreamMetadataInstanceId",
        "bigtableChangeStreamMetadataTableTableId",
        "bigtableChangeStreamCharset",
        "bigtableChangeStreamStartTimestamp",
        "bigtableChangeStreamIgnoreColumnFamilies",
        "bigtableChangeStreamIgnoreColumns",
        "bigtableChangeStreamName",
        "bigtableChangeStreamResume",
        "bigtableReadChangeStreamTimeoutMs",
        "bigtableReadProjectId"
      ]
    }
  },
  {
    "template_name": "Bigtable_Change_Streams_to_PubSub",
    "description": "Streaming pipeline. Streams Bigtable data change records and writes them into PubSub using Dataflow Runner V2.",
    "template_path": "v2/googlecloud-to-googlecloud",
    "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Bigtable_Change_Streams_to_PubSub",
    "type": "FLEX",
    "params": {
      "required": [
        "pubSubTopic",
        "bigtableChangeStreamAppProfile",
        "bigtableReadInstanceId",
        "bigtableReadTableId"
      ],
      "optional": [
        "messageEncoding",
        "messageFormat",
        "stripValues",
        "dlqDirectory",
        "dlqRetryMinutes",
        "dlqMaxRetries",
        "useBase64Rowkeys",
        "pubSubProjectId",
        "useBase64ColumnQualifiers",
        "useBase64Values",
        "disableDlqRetries",
        "bigtableChangeStreamMetadataInstanceId",
        "bigtableChangeStreamMetadataTableTableId",
        "bigtableChangeStreamCharset",
        "bigtableChangeStreamStartTimestamp",
        "bigtableChangeStreamIgnoreColumnFamilies",
        "bigtableChangeStreamIgnoreColumns",
        "bigtableChangeStreamName",
        "bigtableChangeStreamResume",
        "bigtableReadChangeStreamTimeoutMs",
        "bigtableReadProjectId"
      ]
    }
  },
  {
    "template_name": "Bigtable_Change_Streams_to_Google_Cloud_Storage",
    "description": "Streaming pipeline. Streams Bigtable change stream data records and writes them into a Cloud Storage bucket using Dataflow Runner V2.",
    "template_path": "v2/googlecloud-to-googlecloud",
    "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Bigtable_Change_Streams_to_Google_Cloud_Storage",
    "type": "FLEX",
    "params": {
      "required": [
        "gcsOutputDirectory",
        "bigtableChangeStreamAppProfile",
        "bigtableReadInstanceId",
        "bigtableReadTableId"
      ],
      "optional": [
        "outputFileFormat",
        "windowDuration",
        "bigtableMetadataTableTableId",
        "schemaOutputFormat",
        "outputFilenamePrefix",
        "outputBatchSize",
        "outputShardsCount",
        "useBase64Rowkeys",
        "useBase64ColumnQualifiers",
        "useBase64Values",
        "bigtableChangeStreamMetadataInstanceId",
        "bigtableChangeStreamMetadataTableTableId",
        "bigtableChangeStreamCharset",
        "bigtableChangeStreamStartTimestamp",
        "bigtableChangeStreamIgnoreColumnFamilies",
        "bigtableChangeStreamIgnoreColumns",
        "bigtableChangeStreamName",
        "bigtableChangeStreamResume",
        "bigtableReadChangeStreamTimeoutMs",
        "bigtableReadProjectId"
      ]
  }
},
{
  "template_name": "Spanner_Change_Streams_to_BigQuery",
    "description": "The Cloud Spanner change streams to BigQuery template is a streaming pipeline that streams Cloud Spanner data change records and writes them into BigQuery tables using Dataflow Runner V2.",
    "template_path": "v2/googlecloud-to-googlecloud",
    "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Spanner_Change_Streams_to_BigQuery",
    "type": "FLEX",
    "params": {
      "required": [
        "spannerInstanceId",
        "spannerDatabase",
        "spannerMetadataInstanceId",
        "spannerMetadataDatabase",
        "spannerChangeStreamName",
        "bigQueryDataset"
      ],
      "optional": [
        "spannerProjectId",
        "spannerDatabaseRole",
        "spannerMetadataTableName",
        "rpcPriority",
        "spannerHost",
        "startTimestamp",
        "endTimestamp",
        "bigQueryProjectId",
        "bigQueryChangelogTableNameTemplate",
        "deadLetterQueueDirectory",
        "dlqRetryMinutes",
        "ignoreFields",
        "disableDlqRetries",
        "useStorageWriteApi",
        "useStorageWriteApiAtLeastOnce",
        "numStorageWriteApiStreams",
      "storageWriteApiTriggeringFrequencySec"
    ]
  }
},
{
  "template_name": "Spanner_Change_Streams_to_Google_Cloud_Storage",
  "description": "The Cloud Spanner change streams to Cloud Storage template is a streaming pipeline that streams Spanner data change records and writes them into a Cloud Storage bucket using Dataflow Runner V2.",
  "template_path": "v2/googlecloud-to-googlecloud",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Spanner_Change_Streams_to_Google_Cloud_Storage",
  "type": "FLEX",
  "params": {
    "required": [
      "spannerInstanceId",
      "spannerDatabase",
      "spannerMetadataInstanceId",
      "spannerMetadataDatabase",
      "spannerChangeStreamName",
      "gcsOutputDirectory"
    ],
    "optional": [
      "spannerProjectId",
      "spannerDatabaseRole",
      "spannerMetadataTableName",
      "startTimestamp",
      "endTimestamp",
      "spannerHost",
      "outputFileFormat",
      "windowDuration",
      "rpcPriority",
      "outputFilenamePrefix",
      "numShards"
    ]
  }
},
{
  "template_name": "Spanner_Change_Streams_to_Google_Cloud_Storage",
  "description": "The Cloud Spanner change streams to Cloud Storage template is a streaming pipeline that streams Spanner data change records and writes them into a Cloud Storage bucket using Dataflow Runner V2.",
  "template_path": "v2/googlecloud-to-googlecloud",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Spanner_Change_Streams_to_Google_Cloud_Storage",
  "type": "FLEX",
  "params": {
    "required": [
      "spannerInstanceId",
      "spannerDatabase",
      "spannerMetadataInstanceId",
      "spannerMetadataDatabase",
      "spannerChangeStreamName",
      "gcsOutputDirectory"
    ],
    "optional": [
      "spannerProjectId",
      "spannerDatabaseRole",
      "spannerMetadataTableName",
      "startTimestamp",
      "endTimestamp",
      "spannerHost",
      "outputFileFormat",
      "windowDuration",
      "rpcPriority",
      "outputFilenamePrefix",
      "numShards"
    ]
  }
},
{
  "template_name": "Spanner_Change_Streams_to_PubSub",
  "description": "The Cloud Spanner change streams to the Pub/Sub template is a streaming pipeline that streams Cloud Spanner data change records and writes them into Pub/Sub topics using Dataflow Runner V2.",
  "template_path": "v2/googlecloud-to-googlecloud",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Spanner_Change_Streams_to_PubSub",
  "type": "FLEX",
  "params": {
    "required": [
      "spannerInstanceId",
      "spannerDatabase",
      "spannerMetadataInstanceId",
      "spannerMetadataDatabase",
      "spannerChangeStreamName",
      "pubsubTopic"
    ],
    "optional": [
      "spannerProjectId",
      "spannerDatabaseRole",
      "spannerMetadataTableName",
      "startTimestamp",
      "endTimestamp",
      "spannerHost",
      "outputDataFormat",
      "pubsubAPI",
      "pubsubProjectId",
      "rpcPriority",
      "includeSpannerSource",
      "outputMessageMetadata"
    ]
  }
},
{
  "template_name": "Stream_GCS_Text_to_BigQuery_Flex",
  "description": "The Text Files on Cloud Storage to BigQuery pipeline is a streaming pipeline that allows you to stream text files stored in Cloud Storage, transform them using a JavaScript User Defined Function (UDF) that you provide, and append the result to BigQuery.",
  "template_path": "v2/googlecloud-to-googlecloud",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Stream_GCS_Text_to_BigQuery_Flex",
  "type": "FLEX",
  "params": {
    "required": [
      "inputFilePattern",
      "JSONPath",
      "outputTable",
      "javascriptTextTransformGcsPath",
      "javascriptTextTransformFunctionName",
      "bigQueryLoadingTemporaryDirectory"
    ],
    "optional": [
      "outputDeadletterTable",
      "useStorageWriteApiAtLeastOnce",
      "useStorageWriteApi",
      "numStorageWriteApiStreams",
      "storageWriteApiTriggeringFrequencySec",
      "pythonExternalTextTransformGcsPath",
      "javascriptTextTransformReloadIntervalMinutes"
    ]
  }
},
{
  "template_name": "Stream_GCS_Text_to_BigQuery",
  "description": "The Text Files on Cloud Storage to BigQuery pipeline is a streaming pipeline that allows you to stream text files stored in Cloud Storage, transform them using a JavaScript User Defined Function (UDF) that you provide, and append the result to BigQuery.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/Stream_GCS_Text_to_BigQuery",
  "type": "CLASSIC",
  "params": {
    "required": [
      "inputFilePattern",
      "JSONPath",
      "outputTable",
      "bigQueryLoadingTemporaryDirectory"
    ],
    "optional": [
      "outputDeadletterTable",
      "javascriptTextTransformGcsPath",
      "javascriptTextTransformFunctionName",
      "javascriptTextTransformReloadIntervalMinutes"
    ]
  }
},
{
  "template_name": "Stream_GCS_Text_to_BigQuery_Xlang",
  "description": "The Text Files on Cloud Storage to BigQuery pipeline is a streaming pipeline that allows you to stream text files stored in Cloud Storage, transform them using a Python User Defined Function (UDF) that you provide, and append the result to BigQuery.",
  "template_path": "v2/googlecloud-to-googlecloud",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Stream_GCS_Text_to_BigQuery_Xlang",
  "type": "FLEX",
  "params": {
    "required": [
      "inputFilePattern",
      "JSONPath",
      "outputTable",
      "bigQueryLoadingTemporaryDirectory"
    ],
    "optional": [
      "outputDeadletterTable",
      "useStorageWriteApiAtLeastOnce",
      "useStorageWriteApi",
      "numStorageWriteApiStreams",
      "storageWriteApiTriggeringFrequencySec",
      "pythonExternalTextTransformGcsPath",
      "pythonExternalTextTransformFunctionName"
    ]
  }
},
{
  "template_name": "Stream_DLP_GCS_Text_to_BigQuery_Flex",
  "description": "The Data Masking/Tokenization from Cloud Storage to BigQuery template uses Sensitive Data Protection and creates a streaming pipeline that reads CSV files from a Cloud Storage bucket, calls the Cloud Data Loss Prevention API (part of Sensitive Data Protection) for de-identification, and writes the de-identified data into the specified BigQuery table.",
  "template_path": "v2/googlecloud-to-googlecloud",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Stream_DLP_GCS_Text_to_BigQuery_Flex",
  "type": "FLEX",
  "params": {
    "required": [
      "inputFilePattern",
      "deidentifyTemplateName",
      "datasetName",
      "dlpProjectId"
    ],
    "optional": [
      "inspectTemplateName",
      "batchSize",
      "useStorageWriteApi",
      "useStorageWriteApiAtLeastOnce",
      "numStorageWriteApiStreams",
      "storageWriteApiTriggeringFrequencySec"
    ]
  }
},
{
  "template_name": "Cloud_Datastream_to_BigQuery",
  "description": "The Datastream to BigQuery template is a streaming pipeline that reads Datastream data and replicates it into BigQuery. The template reads data from Cloud Storage using Pub/Sub notifications and replicates it into a time partitioned BigQuery staging table. Following replication, the template executes a MERGE in BigQuery to upsert all change data capture (CDC) changes into a replica of the source table.",
  "template_path": "v2/datastream-to-bigquery",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Cloud_Datastream_to_BigQuery",
  "type": "FLEX",
  "params": {
    "required": [
      "inputFilePattern",
      "inputFileFormat",
      "gcsPubSubSubscription",
      "outputStagingDatasetTemplate",
      "outputDatasetTemplate",
      "deadLetterQueueDirectory"
    ],
    "optional": [
      "streamName",
      "rfcStartDateTime",
      "fileReadConcurrency",
      "outputProjectId",
      "outputStagingTableNameTemplate",
      "outputTableNameTemplate",
      "ignoreFields",
      "mergeFrequencyMinutes",
      "dlqRetryMinutes",
      "dataStreamRootUrl",
      "applyMerge",
      "mergeConcurrency",
      "partitionRetentionDays",
      "useStorageWriteApiAtLeastOnce",
      "datastreamSourceType",
      "javascriptTextTransformGcsPath",
      "javascriptTextTransformFunctionName",
      "javascriptTextTransformReloadIntervalMinutes",
      "pythonTextTransformGcsPath",
      "pythonRuntimeVersion",
      "pythonTextTransformFunctionName",
      "runtimeRetries",
      "useStorageWriteApi",
      "numStorageWriteApiStreams",
      "storageWriteApiTriggeringFrequencySec"
    ]
  }
},
{
  "template_name": "Cloud_Datastream_to_Spanner",
  "description": "The Datastream to Cloud Spanner template is a streaming pipeline that reads Datastream events from a Cloud Storage bucket and writes them to a Cloud Spanner database. It is intended for data migration from Datastream sources to Cloud Spanner.",
  "template_path": "v2/datastream-to-spanner",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Cloud_Datastream_to_Spanner",
  "type": "FLEX",
  "params": {
    "required": [
      "instanceId",
      "databaseId"
    ],
    "optional": [
      "inputFilePattern",
      "inputFileFormat",
      "sessionFilePath",
      "projectId",
      "spannerHost",
      "gcsPubSubSubscription",
      "streamName",
      "shadowTablePrefix",
      "shouldCreateShadowTables",
      "rfcStartDateTime",
      "fileReadConcurrency",
      "deadLetterQueueDirectory",
      "dlqRetryMinutes",
      "dlqMaxRetryCount",
      "dataStreamRootUrl",
      "datastreamSourceType",
      "roundJsonDecimals",
      "runMode",
      "transformationContextFilePath",
      "directoryWatchDurationInMinutes",
      "spannerPriority",
      "dlqGcsPubSubSubscription",
      "transformationJarPath",
      "transformationClassName",
      "transformationCustomParameters",
      "filteredEventsDirectory",
      "shardingContextFilePath",
      "tableOverrides",
      "columnOverrides",
      "schemaOverridesFilePath",
      "shadowTableSpannerDatabaseId",
      "shadowTableSpannerInstanceId",
      "failureInjectionParameter"
    ]
  }
},
{
  "template_name": "Cloud_Datastream_MongoDB_to_Firestore",
  "description": "The Datastream MongoDB to Firestore template is a streaming pipeline that reads Datastream events from a Cloud Storage bucket and writes them to a Firestore with MongoDB compatibility database.",
  "template_path": "v2/datastream-mongodb-to-firestore",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Cloud_Datastream_MongoDB_to_Firestore",
  "type": "FLEX",
  "params": {
    "required": [
      "inputFilePattern",
      "inputFileFormat",
      "connectionUri",
      "databaseName"
    ],
    "optional": [
      "rfcStartDateTime",
      "fileReadConcurrency",
      "gcsPubSubSubscription",
      "databaseCollection",
      "shadowCollectionPrefix",
      "batchSize",
      "deadLetterQueueDirectory",
      "dlqRetryMinutes",
      "dlqMaxRetryCount",
      "processBackfillFirst",
      "useShadowTablesForBackfill",
      "runMode",
      "directoryWatchDurationInMinutes",
      "streamName",
      "dlqGcsPubSubSubscription"
    ]
  }
},
{
  "template_name": "Cloud_Datastream_to_SQL",
  "description": "The Datastream to SQL template is a streaming pipeline that reads Datastream data and replicates it into any MySQL or PostgreSQL database.",
  "template_path": "v2/datastream-to-sql",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Cloud_Datastream_to_SQL",
  "type": "FLEX",
  "params": {
    "required": [
      "inputFilePattern",
      "databaseHost",
      "databaseUser",
      "databasePassword"
    ],
    "optional": [
      "gcsPubSubSubscription",
      "inputFileFormat",
      "streamName",
      "rfcStartDateTime",
      "dataStreamRootUrl",
      "databaseType",
      "databasePort",
      "databaseName",
      "defaultCasing",
      "columnCasing",
      "schemaMap",
      "customConnectionString",
      "numThreads",
      "databaseLoginTimeout",
      "datastreamSourceType",
      "orderByIncludesIsDeleted"
    ]
  }
},
{
  "template_name": "Jms_to_PubSub",
  "description": "The JMS to Pub/Sub template is a streaming pipeline that reads messages from ActiveMQ JMS Server (Queue/Topic) and writes them to Pub/Sub.",
  "template_path": "v2/jms-to-pubsub",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Jms_to_PubSub",
  "type": "FLEX",
  "params": {
    "required": [
      "inputName",
      "inputType",
      "outputTopic",
      "username",
      "password"
    ],
    "optional": [
      "jmsServer"
    ]
  }
},
{
  "template_name": "Kafka_to_BigQuery_Flex",
  "description": "The Apache Kafka to BigQuery template is a streaming pipeline which ingests text data from Apache Kafka, and outputs the resulting records to BigQuery. Any errors which occur in the transformation of the data, or inserting into the output table are inserted into a separate errors table in BigQuery. For any errors which occur in the transformation of the data, the original records can be inserted into a separate Kafka topic. The template supports reading a Kafka topic which contains single/multiple schema(s). It can write to a single or multiple BigQuery tables, depending on the schema of records.",
  "template_path": "v2/kafka-to-bigquery",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Kafka_to_BigQuery_Flex",
  "type": "FLEX",
  "params": {
    "required": [
      "readBootstrapServerAndTopic",
      "writeMode",
      "kafkaReadAuthenticationMode",
      "messageFormat",
      "useBigQueryDLQ"
    ],
    "optional": [
      "outputTableSpec",
      "persistKafkaKey",
      "outputProject",
      "outputDataset",
      "bqTableNamePrefix",
      "createDisposition",
      "writeDisposition",
      "useAutoSharding",
      "numStorageWriteApiStreams",
      "storageWriteApiTriggeringFrequencySec",
      "useStorageWriteApiAtLeastOnce",
      "enableCommitOffsets",
      "consumerGroupId",
      "kafkaReadOffset",
      "kafkaReadUsernameSecretId",
      "kafkaReadPasswordSecretId",
      "kafkaReadKeystoreLocation",
      "kafkaReadTruststoreLocation",
      "kafkaReadTruststorePasswordSecretId",
      "kafkaReadKeystorePasswordSecretId",
      "kafkaReadKeyPasswordSecretId",
      "kafkaReadSaslScramUsernameSecretId",
      "kafkaReadSaslScramPasswordSecretId",
      "kafkaReadSaslScramTruststoreLocation",
      "kafkaReadSaslScramTruststorePasswordSecretId",
      "schemaFormat",
      "confluentAvroSchemaPath",
      "schemaRegistryConnectionUrl",
      "binaryAvroSchemaPath",
      "schemaRegistryAuthenticationMode",
      "schemaRegistryTruststoreLocation",
      "schemaRegistryTruststorePasswordSecretId",
      "schemaRegistryKeystoreLocation",
      "schemaRegistryKeystorePasswordSecretId",
      "schemaRegistryKeyPasswordSecretId",
      "schemaRegistryOauthClientId",
      "schemaRegistryOauthClientSecretId",
      "schemaRegistryOauthScope",
      "schemaRegistryOauthTokenEndpointUrl",
      "outputDeadletterTable",
      "javascriptTextTransformGcsPath",
      "javascriptTextTransformFunctionName",
      "javascriptTextTransformReloadIntervalMinutes"
    ]
  }
},
{
  "template_name": "Kafka_to_Gcs_Flex",
  "description": "A streaming pipeline which ingests data from Kafka and writes to a pre-existing Cloud Storage bucket with a variety of file types.",
  "template_path": "v2/kafka-to-gcs",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Kafka_to_Gcs_Flex",
  "type": "FLEX",
  "params": {
    "required": [
      "readBootstrapServerAndTopic",
      "outputDirectory",
      "kafkaReadAuthenticationMode",
      "messageFormat",
      "useBigQueryDLQ"
    ],
    "optional": [
      "windowDuration",
      "outputFilenamePrefix",
      "numShards",
      "enableCommitOffsets",
      "consumerGroupId",
      "kafkaReadOffset",
      "kafkaReadUsernameSecretId",
      "kafkaReadPasswordSecretId",
      "kafkaReadKeystoreLocation",
      "kafkaReadTruststoreLocation",
      "kafkaReadTruststorePasswordSecretId",
      "kafkaReadKeystorePasswordSecretId",
      "kafkaReadKeyPasswordSecretId",
      "kafkaReadSaslScramUsernameSecretId",
      "kafkaReadSaslScramPasswordSecretId",
      "kafkaReadSaslScramTruststoreLocation",
      "kafkaReadSaslScramTruststorePasswordSecretId",
      "schemaFormat",
      "confluentAvroSchemaPath",
      "schemaRegistryConnectionUrl",
      "binaryAvroSchemaPath",
      "schemaRegistryAuthenticationMode",
      "schemaRegistryTruststoreLocation",
      "schemaRegistryTruststorePasswordSecretId",
      "schemaRegistryKeystoreLocation",
      "schemaRegistryKeystorePasswordSecretId",
      "schemaRegistryKeyPasswordSecretId",
      "schemaRegistryOauthClientId",
      "schemaRegistryOauthClientSecretId",
      "schemaRegistryOauthScope",
      "schemaRegistryOauthTokenEndpointUrl",
      "outputDeadletterTable"
    ]
  }
},
{
  "template_name": "Kafka_to_Kafka",
  "description": "A pipeline that writes data to a kafka destination from another kafka source.",
  "template_path": "v2/kafka-to-kafka",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Kafka_to_Kafka",
  "type": "FLEX",
  "params": {
    "required": [
      "readBootstrapServerAndTopic",
      "kafkaReadAuthenticationMode",
      "writeBootstrapServerAndTopic",
      "kafkaWriteAuthenticationMethod"
    ],
    "optional": [
      "enableCommitOffsets",
      "consumerGroupId",
      "kafkaReadOffset",
      "kafkaReadUsernameSecretId",
      "kafkaReadPasswordSecretId",
      "kafkaReadKeystoreLocation",
      "kafkaReadTruststoreLocation",
      "kafkaReadTruststorePasswordSecretId",
      "kafkaReadKeystorePasswordSecretId",
      "kafkaReadKeyPasswordSecretId",
      "kafkaReadSaslScramUsernameSecretId",
      "kafkaReadSaslScramPasswordSecretId",
      "kafkaReadSaslScramTruststoreLocation",
      "kafkaReadSaslScramTruststorePasswordSecretId",
      "kafkaWriteUsernameSecretId",
      "kafkaWritePasswordSecretId",
      "kafkaWriteKeystoreLocation",
      "kafkaWriteTruststoreLocation",
      "kafkaWriteTruststorePasswordSecretId",
      "kafkaWriteKeystorePasswordSecretId",
      "kafkaWriteKeyPasswordSecretId"
    ]
  }
},
{
  "template_name": "Mqtt_to_PubSub",
  "description": "The MQTT to Pub/Sub template is a streaming pipeline that reads messages from an MQTT topic and writes them to Pub/Sub. It includes the optional parameters username and password in case authentication is required by the MQTT server.",
  "template_path": "v2/mqtt-to-pubsub",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Mqtt_to_PubSub",
  "type": "FLEX",
  "params": {
    "required": [
      "inputTopic",
      "outputTopic",
      "username",
      "password"
    ],
    "optional": [
      "brokerServer"
    ]
  }
},
{
  "template_name": "PubSub_Proto_to_BigQuery_Flex",
  "description": "The Pub/Sub proto to BigQuery template is a streaming pipeline that ingests proto data from a Pub/Sub subscription into a BigQuery table. Any errors that occur while writing to the BigQuery table are streamed into a Pub/Sub unprocessed topic.",
  "template_path": "v2/pubsub-binary-to-bigquery",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/PubSub_Proto_to_BigQuery_Flex",
  "type": "FLEX",
  "params": {
    "required": [
      "protoSchemaPath",
      "fullMessageName",
      "inputSubscription",
      "outputTableSpec",
      "outputTopic"
    ],
    "optional": [
      "preserveProtoFieldNames",
      "bigQueryTableSchemaPath",
      "udfOutputTopic",
      "useStorageWriteApiAtLeastOnce",
      "writeDisposition",
      "createDisposition",
      "javascriptTextTransformGcsPath",
      "javascriptTextTransformFunctionName",
      "javascriptTextTransformReloadIntervalMinutes",
      "useStorageWriteApi",
      "numStorageWriteApiStreams",
      "storageWriteApiTriggeringFrequencySec"
    ]
  }
},
{
  "template_name": "PubSub_Proto_to_BigQuery_Xlang",
  "description": "The Pub/Sub proto to BigQuery template is a streaming pipeline that ingests proto data from a Pub/Sub subscription into a BigQuery table. Any errors that occur while writing to the BigQuery table are streamed into a Pub/Sub unprocessed topic. A Python user-defined function (UDF) can be provided to transform data. Errors while executing the UDF can be sent to either a separate Pub/Sub topic or the same unprocessed topic as the BigQuery errors.",
  "template_path": "v2/pubsub-binary-to-bigquery",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/PubSub_Proto_to_BigQuery_Xlang",
  "type": "FLEX",
  "params": {
    "required": [
      "protoSchemaPath",
      "fullMessageName",
      "inputSubscription",
      "outputTableSpec",
      "outputTopic"
    ],
    "optional": [
      "preserveProtoFieldNames",
      "bigQueryTableSchemaPath",
      "udfOutputTopic",
      "useStorageWriteApiAtLeastOnce",
      "writeDisposition",
      "createDisposition",
      "pythonExternalTextTransformGcsPath",
      "pythonExternalTextTransformFunctionName",
      "useStorageWriteApi",
      "numStorageWriteApiStreams",
      "storageWriteApiTriggeringFrequencySec"
    ]
  }
},
{
  "template_name": "Cloud_PubSub_to_GCS_Text_Flex",
  "description": "The Pub/Sub Topic or Subscription to Cloud Storage Text template is a streaming pipeline that reads records from Pub/Sub and saves them as a series of Cloud Storage files in text format.",
  "template_path": "v2/googlecloud-to-googlecloud",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Cloud_PubSub_to_GCS_Text_Flex",
  "type": "FLEX",
  "params": {
    "required": [
      "outputDirectory"
    ],
    "optional": [
      "inputTopic",
      "inputSubscription",
      "userTempLocation",
      "outputFilenamePrefix",
      "outputFilenameSuffix",
      "outputShardTemplate",
      "numShards",
      "windowDuration",
      "yearPattern",
      "monthPattern",
      "dayPattern",
      "hourPattern",
      "minutePattern"
    ]
  }
},
{
  "template_name": "PubSub_Subscription_to_BigQuery",
  "description": "The Pub/Sub Subscription to BigQuery template is a streaming pipeline that reads JSON-formatted messages from a Pub/Sub subscription and writes them to a BigQuery table. You can use the template as a quick solution to move Pub/Sub data to BigQuery. The template reads JSON-formatted messages from Pub/Sub and converts them to BigQuery elements.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/PubSub_Subscription_to_BigQuery",
  "type": "CLASSIC",
  "params": {
    "required": [
      "outputTableSpec",
      "inputSubscription"
    ],
    "optional": [
      "outputDeadletterTable",
      "javascriptTextTransformGcsPath",
      "javascriptTextTransformFunctionName",
      "javascriptTextTransformReloadIntervalMinutes"
    ]
  }
},
{
  "template_name": "Cloud_PubSub_to_Avro",
  "description": "The Pub/Sub to Avro files on Cloud Storage template is a streaming pipeline that reads data from a Pub/Sub topic and writes Avro files into the specified Cloud Storage bucket.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/Cloud_PubSub_to_Avro",
  "type": "CLASSIC",
  "params": {
    "required": [
      "inputTopic",
      "outputDirectory",
      "avroTempDirectory"
    ],
    "optional": [
      "outputFilenamePrefix",
      "outputFilenameSuffix",
      "outputShardTemplate",
      "yearPattern",
      "monthPattern",
      "dayPattern",
      "hourPattern",
      "minutePattern"
    ]
  }
},
{
  "template_name": "PubSub_to_BigQuery_Flex",
  "description": "The Pub/Sub to BigQuery template is a streaming pipeline that reads JSON-formatted messages from a Pub/Sub topic or subscription, and writes them to a BigQuery table. You can use the template as a quick solution to move Pub/Sub data to BigQuery. The template reads JSON-formatted messages from Pub/Sub and converts them to BigQuery elements.",
  "template_path": "v2/googlecloud-to-googlecloud",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/PubSub_to_BigQuery_Flex",
  "type": "FLEX",
  "params": {
    "required": [
      "outputTableSpec"
    ],
    "optional": [
      "inputTopic",
      "inputSubscription",
      "outputDeadletterTable",
      "useStorageWriteApiAtLeastOnce",
      "useStorageWriteApi",
      "numStorageWriteApiStreams",
      "storageWriteApiTriggeringFrequencySec",
      "javascriptTextTransformGcsPath",
      "javascriptTextTransformFunctionName",
      "javascriptTextTransformReloadIntervalMinutes"
    ]
  }
},
{
  "template_name": "PubSub_to_BigQuery_Xlang",
  "description": "The Pub/Sub to BigQuery template is a streaming pipeline that reads JSON-formatted messages from a Pub/Sub topic or subscription, and writes them to a BigQuery table. You can use the template as a quick solution to move Pub/Sub data to BigQuery. The template reads JSON-formatted messages from Pub/Sub and converts them to BigQuery elements.",
  "template_path": "v2/googlecloud-to-googlecloud",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/PubSub_to_BigQuery_Xlang",
  "type": "FLEX",
  "params": {
    "required": [
      "outputTableSpec"
    ],
    "optional": [
      "inputTopic",
      "inputSubscription",
      "outputDeadletterTable",
      "useStorageWriteApiAtLeastOnce",
      "useStorageWriteApi",
      "numStorageWriteApiStreams",
      "storageWriteApiTriggeringFrequencySec",
      "pythonExternalTextTransformGcsPath",
      "pythonExternalTextTransformFunctionName"
    ]
  }
},
{
  "template_name": "Cloud_PubSub_to_Datadog",
  "description": "The Pub/Sub to Datadog template is a streaming pipeline that reads messages from a Pub/Sub subscription and writes the message payload to Datadog by using a Datadog endpoint.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/Cloud_PubSub_to_Datadog",
  "type": "CLASSIC",
  "params": {
    "required": [
      "inputSubscription",
      "url",
      "outputDeadletterTopic"
    ],
    "optional": [
      "apiKey",
      "batchCount",
      "parallelism",
      "includePubsubMessage",
      "apiKeyKMSEncryptionKey",
      "apiKeySecretId",
      "apiKeySource",
      "javascriptTextTransformGcsPath",
      "javascriptTextTransformFunctionName",
      "javascriptTextTransformReloadIntervalMinutes"
    ]
  }
},
{
  "template_name": "PubSub_to_Elasticsearch_Flex",
  "description": "The Pub/Sub to Elasticsearch template is a streaming pipeline that reads messages from a Pub/Sub subscription, executes a user-defined function (UDF), and writes them to Elasticsearch as documents.",
  "template_path": "v2/googlecloud-to-elasticsearch",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/PubSub_to_Elasticsearch_Flex",
  "type": "FLEX",
  "params": {
    "required": [
      "inputSubscription",
      "errorOutputTopic",
      "connectionUrl",
      "apiKey"
    ],
    "optional": [
      "dataset",
      "namespace",
      "elasticsearchTemplateVersion",
      "javascriptTextTransformGcsPath",
      "javascriptTextTransformFunctionName",
      "javascriptTextTransformReloadIntervalMinutes",
      "elasticsearchUsername",
      "elasticsearchPassword",
      "batchSize",
      "batchSizeBytes",
      "maxRetryAttempts",
      "maxRetryDuration",
      "propertyAsIndex",
      "javaScriptIndexFnGcsPath",
      "javaScriptIndexFnName",
      "propertyAsId",
      "javaScriptIdFnGcsPath",
      "javaScriptIdFnName",
      "javaScriptTypeFnGcsPath",
      "javaScriptTypeFnName",
      "javaScriptIsDeleteFnGcsPath",
      "javaScriptIsDeleteFnName",
      "usePartialUpdate",
      "bulkInsertMethod",
      "trustSelfSignedCerts",
      "disableCertificateValidation",
      "apiKeyKMSEncryptionKey",
      "apiKeySecretId",
      "apiKeySource",
      "socketTimeout"
    ]
  }
},
{
  "template_name": "PubSub_to_Elasticsearch_Xlang",
  "description": "The Pub/Sub to Elasticsearch template is a streaming pipeline that reads messages from a Pub/Sub subscription, executes a Python user-defined function (UDF), and writes them to Elasticsearch as documents.",
  "template_path": "v2/googlecloud-to-elasticsearch",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/PubSub_to_Elasticsearch_Xlang",
  "type": "FLEX",
  "params": {
    "required": [
      "inputSubscription",
      "errorOutputTopic",
      "connectionUrl",
      "apiKey"
    ],
    "optional": [
      "dataset",
      "namespace",
      "elasticsearchTemplateVersion",
      "pythonExternalTextTransformGcsPath",
      "pythonExternalTextTransformFunctionName",
      "elasticsearchUsername",
      "elasticsearchPassword",
      "batchSize",
      "batchSizeBytes",
      "maxRetryAttempts",
      "maxRetryDuration",
      "propertyAsIndex",
      "javaScriptIndexFnGcsPath",
      "javaScriptIndexFnName",
      "propertyAsId",
      "javaScriptIdFnGcsPath",
      "javaScriptIdFnName",
      "javaScriptTypeFnGcsPath",
      "javaScriptTypeFnName",
      "javaScriptIsDeleteFnGcsPath",
      "javaScriptIsDeleteFnName",
      "usePartialUpdate",
      "bulkInsertMethod",
      "trustSelfSignedCerts",
      "disableCertificateValidation",
      "apiKeyKMSEncryptionKey",
      "apiKeySecretId",
      "apiKeySource",
      "socketTimeout"
    ]
  }
},
{
  "template_name": "Pubsub_to_Jdbc",
  "description": "The Pub/Sub to Java Database Connectivity (JDBC) template is a streaming pipeline that ingests data from a pre-existing Cloud Pub/Sub subscription as JSON strings, and writes the resulting records to JDBC.",
  "template_path": "v2/jdbc-and-googlecloud",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Pubsub_to_Jdbc",
  "type": "FLEX",
  "params": {
    "required": [
      "inputSubscription",
      "driverClassName",
      "connectionUrl",
      "driverJars",
      "statement",
      "outputDeadletterTopic"
    ],
    "optional": [
      "username",
      "password",
      "connectionProperties",
      "KMSEncryptionKey",
      "disabledAlgorithms",
      "extraFilesToStage"
    ]
  }
},
{
  "template_name": "Cloud_PubSub_to_MongoDB",
  "description": "The Pub/Sub to MongoDB template is a streaming pipeline that reads JSON-encoded messages from a Pub/Sub subscription and writes them to MongoDB as documents.",
  "template_path": "v2/pubsub-to-mongodb",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Cloud_PubSub_to_MongoDB",
  "type": "FLEX",
  "params": {
    "required": [
      "inputSubscription",
      "mongoDBUri",
      "database",
      "collection",
      "deadletterTable"
    ],
    "optional": [
      "batchSize",
      "batchSizeBytes",
      "maxConnectionIdleTime",
      "sslEnabled",
      "ignoreSSLCertificate",
      "withOrdered",
      "withSSLInvalidHostNameAllowed",
      "javascriptTextTransformGcsPath",
      "javascriptTextTransformFunctionName",
      "javascriptTextTransformReloadIntervalMinutes"
    ]
  }
},
{
  "template_name": "Cloud_PubSub_to_MongoDB_Xlang",
  "description": "The Pub/Sub to MongoDB template is a streaming pipeline that reads JSON-encoded messages from a Pub/Sub subscription and writes them to MongoDB as documents. If required, this pipeline supports additional transforms that can be included using a Python user-defined function (UDF). Any errors occurred due to schema mismatch, malformed JSON, or while executing transforms are recorded in a BigQuery table for unprocessed messages along with input message. If a table for unprocessed records does not exist prior to execution, the pipeline automatically creates this table.",
  "template_path": "v2/pubsub-to-mongodb",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Cloud_PubSub_to_MongoDB_Xlang",
  "type": "FLEX",
  "params": {
    "required": [
      "inputSubscription",
      "mongoDBUri",
      "database",
      "collection",
      "deadletterTable"
    ],
    "optional": [
      "batchSize",
      "batchSizeBytes",
      "maxConnectionIdleTime",
      "sslEnabled",
      "ignoreSSLCertificate",
      "withOrdered",
      "withSSLInvalidHostNameAllowed",
      "pythonExternalTextTransformGcsPath",
      "pythonExternalTextTransformFunctionName"
    ]
  }
},
{
  "template_name": "Cloud_PubSub_to_Cloud_PubSub",
  "description": "The Pub/Sub to Pub/Sub template is a streaming pipeline that reads messages from a Pub/Sub subscription and writes the messages to another Pub/Sub topic. The pipeline also accepts an optional message attribute key and a value that can be used to filter the messages that should be written to the Pub/Sub topic. You can use this template to copy messages from a Pub/Sub subscription to another Pub/Sub topic with an optional message filter.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/Cloud_PubSub_to_Cloud_PubSub",
  "type": "CLASSIC",
  "params": {
    "required": [
      "inputSubscription",
      "outputTopic"
    ],
    "optional": [
      "filterKey",
      "filterValue"
    ]
  }
},
{
  "template_name": "Cloud_PubSub_to_Redis",
  "description": "The Pub/Sub to Redis template is a streaming pipeline that reads messages from a Pub/Sub subscription and writes the message payload to Redis.",
  "template_path": "v2/pubsub-to-redis",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Cloud_PubSub_to_Redis",
  "type": "FLEX",
  "params": {
    "required": [
      "inputSubscription",
      "redisHost",
      "redisPort",
      "redisPassword"
    ],
    "optional": [
      "sslEnabled",
      "redisSinkType",
      "connectionTimeout",
      "ttl",
      "javascriptTextTransformGcsPath",
      "javascriptTextTransformFunctionName",
      "javascriptTextTransformReloadIntervalMinutes"
    ]
  }
},
{
  "template_name": "Cloud_PubSub_to_Splunk",
  "description": "The Pub/Sub to Splunk template is a streaming pipeline that reads messages from a Pub/Sub subscription and writes the message payload to Splunk via Splunk's HTTP Event Collector (HEC).",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/Cloud_PubSub_to_Splunk",
  "type": "CLASSIC",
  "params": {
    "required": [
      "inputSubscription",
      "url",
      "outputDeadletterTopic"
    ],
    "optional": [
      "token",
      "batchCount",
      "disableCertificateValidation",
      "parallelism",
      "includePubsubMessage",
      "tokenKMSEncryptionKey",
      "tokenSecretId",
      "tokenSource",
      "rootCaCertificatePath",
      "enableBatchLogs",
      "enableGzipHttpCompression",
      "javascriptTextTransformGcsPath",
      "javascriptTextTransformFunctionName",
      "javascriptTextTransformReloadIntervalMinutes"
    ]
  }
},
{
  "template_name": "Cloud_PubSub_to_GCS_Text",
  "description": "The Pub/Sub to Cloud Storage Text template is a streaming pipeline that reads records from Pub/Sub topic and saves them as a series of Cloud Storage files in text format.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/Cloud_PubSub_to_GCS_Text",
  "type": "CLASSIC",
  "params": {
    "required": [
      "outputDirectory",
      "outputFilenamePrefix"
    ],
    "optional": [
      "inputTopic",
      "userTempLocation",
      "outputFilenameSuffix",
      "outputShardTemplate",
      "yearPattern",
      "monthPattern",
      "dayPattern",
      "hourPattern",
      "minutePattern"
    ]
  }
},
{
  "template_name": "Spanner_to_SourceDb",
  "description": "Streaming pipeline. Reads data from Spanner Change Streams and writes them to a source.",
  "template_path": "v2/spanner-to-sourcedb",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Spanner_to_SourceDb",
  "type": "FLEX",
  "params": {
    "required": [
      "changeStreamName",
      "instanceId",
      "databaseId",
      "spannerProjectId",
      "metadataInstance",
      "metadataDatabase",
      "sourceShardsFilePath"
    ],
    "optional": [
      "startTimestamp",
      "endTimestamp",
      "shadowTablePrefix",
      "sessionFilePath",
      "filtrationMode",
      "shardingCustomJarPath",
      "shardingCustomClassName",
      "shardingCustomParameters",
      "sourceDbTimezoneOffset",
      "dlqGcsPubSubSubscription",
      "skipDirectoryName",
      "maxShardConnections",
      "deadLetterQueueDirectory",
      "dlqMaxRetryCount",
      "runMode",
      "dlqRetryMinutes",
      "sourceType",
      "transformationJarPath",
      "transformationClassName",
      "transformationCustomParameters",
      "tableOverrides",
      "columnOverrides",
      "schemaOverridesFilePath",
      "filterEventsDirectoryName",
      "isShardedMigration",
      "failureInjectionParameter",
      "spannerPriority"
    ]
  }
},
{
  "template_name": "Cdc_To_BigQuery_Template",
  "description": "A pipeline to synchronize a Change Data Capture streams to BigQuery.",
  "template_path": "v2/cdc-parent/cdc-change-applier",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Cdc_To_BigQuery_Template",
  "type": "FLEX",
  "params": {
    "required": [
      "inputSubscriptions",
      "changeLogDataset",
      "replicaDataset"
    ],
    "optional": [
      "inputTopics",
      "updateFrequencySecs",
      "useSingleTopic",
      "useStorageWriteApi",
      "useStorageWriteApiAtLeastOnce",
      "numStorageWriteApiStreams",
      "storageWriteApiTriggeringFrequencySec"
    ]
  }
},
{
  "template_name": "Stream_GCS_Text_to_Cloud_PubSub",
  "description": "This template creates a streaming pipeline that continuously polls for new text files uploaded to Cloud Storage, reads each file line by line, and publishes strings to a Pub/Sub topic.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/Stream_GCS_Text_to_Cloud_PubSub",
  "type": "CLASSIC",
  "params": {
    "required": [
      "inputFilePattern",
      "outputTopic"
    ],
    "optional": []
  }
},
{
  "template_name": "AstraDB_To_BigQuery",
  "description": "The AstraDB to BigQuery template is a batch pipeline that reads records from AstraDB and writes them to BigQuery.",
  "template_path": "v2/astradb-to-bigquery",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/AstraDB_To_BigQuery",
  "type": "FLEX",
  "params": {
    "required": [
      "astraToken",
      "astraDatabaseId",
      "astraKeyspace",
      "astraTable"
    ],
    "optional": [
      "astraQuery",
      "astraDatabaseRegion",
      "minTokenRangesCount",
      "outputTableSpec"
    ]
  }
},
{
  "template_name": "GCS_Avro_to_Cloud_Bigtable",
  "description": "The Cloud Storage Avro to Bigtable template is a pipeline that reads data from Avro files in a Cloud Storage bucket and writes the data to a Bigtable table. You can use the template to copy data from Cloud Storage to Bigtable.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/GCS_Avro_to_Cloud_Bigtable",
  "type": "CLASSIC",
  "params": {
    "required": [
      "bigtableProjectId",
      "bigtableInstanceId",
      "bigtableTableId",
      "inputFilePattern"
    ],
    "optional": [
      "splitLargeRows"
    ]
  }
},
{
  "template_name": "GCS_Avro_to_Cloud_Spanner",
  "description": "The Cloud Storage Avro files to Cloud Spanner template is a batch pipeline that reads Avro files exported from Cloud Spanner stored in Cloud Storage and imports them to a Cloud Spanner database.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/GCS_Avro_to_Cloud_Spanner",
  "type": "CLASSIC",
  "params": {
    "required": [
      "instanceId",
      "databaseId",
      "inputDir"
    ],
    "optional": [
      "spannerHost",
      "waitForIndexes",
      "waitForForeignKeys",
      "waitForChangeStreams",
      "waitForSequences",
      "earlyIndexCreateFlag",
      "spannerProjectId",
      "ddlCreationTimeoutInMinutes",
      "spannerPriority",
      "earlyIndexCreateThreshold"
    ]
  }
},
{
  "template_name": "BigQuery_to_Parquet",
  "description": "The BigQuery export to Parquet template is a batch pipeline that reads data from a BigQuery table and writes it to a Cloud Storage bucket in Parquet format. This template utilizes the BigQuery Storage API to export the data.",
  "template_path": "v2/bigquery-to-parquet",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/BigQuery_to_Parquet",
  "type": "FLEX",
  "params": {
    "required": [
      "tableRef",
      "bucket"
    ],
    "optional": [
      "numShards",
      "fields",
      "rowRestriction"
    ]
  }
},
{
  "template_name": "BigQuery_to_Bigtable",
  "description": "A pipeline to export a BigQuery table into Bigtable.",
  "template_path": "v2/bigquery-to-bigtable",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/BigQuery_to_Bigtable",
  "type": "FLEX",
  "params": {
    "required": [
      "readIdColumn",
      "bigtableWriteInstanceId",
      "bigtableWriteTableId",
      "bigtableWriteColumnFamily"
    ],
    "optional": [
      "timestampColumn",
      "skipNullValues",
      "inputTableSpec",
      "outputDeadletterTable",
      "query",
      "useLegacySql",
      "queryLocation",
      "queryTempDataset",
      "KMSEncryptionKey",
      "bigtableRpcAttemptTimeoutMs",
      "bigtableRpcTimeoutMs",
      "bigtableAdditionalRetryCodes",
      "bigtableWriteAppProfile",
      "bigtableWriteProjectId",
      "bigtableBulkWriteLatencyTargetMs",
      "bigtableBulkWriteMaxRowKeyCount",
      "bigtableBulkWriteMaxRequestSizeBytes"
    ]
  }
},
{
  "template_name": "BigQuery_to_ClickHouse",
  "description": "The BigQuery to ClickHouse template is a batch pipeline that ingests data from a BigQuery table into ClickHouse table. The template can either read the entire table or read specific records using a supplied query.",
  "template_path": "v2/googlecloud-to-clickhouse",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/BigQuery_to_ClickHouse",
  "type": "FLEX",
  "params": {
    "required": [
      "jdbcUrl",
      "clickHouseUsername",
      "clickHouseTable"
    ],
    "optional": [
      "inputTableSpec",
      "outputDeadletterTable",
      "query",
      "useLegacySql",
      "queryLocation",
      "queryTempDataset",
      "KMSEncryptionKey",
      "clickHousePassword",
      "maxInsertBlockSize",
      "insertDistributedSync",
      "insertQuorum",
      "insertDeduplicate",
      "maxRetries"
    ]
  }
},
{
  "template_name": "BigQuery_to_Elasticsearch",
  "description": "The BigQuery to Elasticsearch template is a batch pipeline that ingests data from a BigQuery table into Elasticsearch as documents. The template can either read the entire table or read specific records using a supplied query.",
  "template_path": "v2/googlecloud-to-elasticsearch",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/BigQuery_to_Elasticsearch",
  "type": "FLEX",
  "params": {
    "required": [
      "connectionUrl",
      "apiKey",
      "index"
    ],
    "optional": [
      "inputTableSpec",
      "outputDeadletterTable",
      "query",
      "useLegacySql",
      "queryLocation",
      "queryTempDataset",
      "KMSEncryptionKey",
      "elasticsearchUsername",
      "elasticsearchPassword",
      "batchSize",
      "batchSizeBytes",
      "maxRetryAttempts",
      "maxRetryDuration",
      "propertyAsIndex",
      "javaScriptIndexFnGcsPath",
      "javaScriptIndexFnName",
      "propertyAsId",
      "javaScriptIdFnGcsPath",
      "javaScriptIdFnName",
      "javaScriptTypeFnGcsPath",
      "javaScriptTypeFnName",
      "javaScriptIsDeleteFnGcsPath",
      "javaScriptIsDeleteFnName",
      "usePartialUpdate",
      "bulkInsertMethod",
      "trustSelfSignedCerts",
      "disableCertificateValidation",
      "apiKeyKMSEncryptionKey",
      "apiKeySecretId",
      "apiKeySource",
      "socketTimeout",
      "javascriptTextTransformGcsPath",
      "javascriptTextTransformFunctionName"
    ]
  }
},
{
  "template_name": "BigQuery_to_Elasticsearch_Xlang",
  "description": "The BigQuery to Elasticsearch template is a batch pipeline that ingests data from a BigQuery table into Elasticsearch as documents. The template can either read the entire table or read specific records using a supplied query.",
  "template_path": "v2/googlecloud-to-elasticsearch",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/BigQuery_to_Elasticsearch_Xlang",
  "type": "FLEX",
  "params": {
    "required": [
      "connectionUrl",
      "apiKey",
      "index"
    ],
    "optional": [
      "inputTableSpec",
      "outputDeadletterTable",
      "query",
      "useLegacySql",
      "queryLocation",
      "queryTempDataset",
      "KMSEncryptionKey",
      "elasticsearchUsername",
      "elasticsearchPassword",
      "batchSize",
      "batchSizeBytes",
      "maxRetryAttempts",
      "maxRetryDuration",
      "propertyAsIndex",
      "javaScriptIndexFnGcsPath",
      "javaScriptIndexFnName",
      "propertyAsId",
      "javaScriptIdFnGcsPath",
      "javaScriptIdFnName",
      "javaScriptTypeFnGcsPath",
      "javaScriptTypeFnName",
      "javaScriptIsDeleteFnGcsPath",
      "javaScriptIsDeleteFnName",
      "usePartialUpdate",
      "bulkInsertMethod",
      "trustSelfSignedCerts",
      "disableCertificateValidation",
      "apiKeyKMSEncryptionKey",
      "apiKeySecretId",
      "apiKeySource",
      "socketTimeout",
      "pythonExternalTextTransformGcsPath",
      "pythonExternalTextTransformFunctionName"
    ]
  }
},
{
  "template_name": "BigQuery_to_MongoDB",
  "description": "The BigQuery to MongoDB template is a batch pipeline that reads rows from a BigQuery and writes them to MongoDB as documents. Currently each row is stored as a document.",
  "template_path": "v2/googlecloud-to-mongodb",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/BigQuery_to_MongoDB",
  "type": "FLEX",
  "params": {
    "required": [
      "mongoDbUri",
      "database",
      "collection",
      "inputTableSpec"
    ],
    "optional": []
  }
},
{
  "template_name": "Cloud_BigQuery_to_GCS_TensorFlow_Records",
  "description": "The BigQuery to Cloud Storage TFRecords template is a pipeline that reads data from a BigQuery query and writes it to a Cloud Storage bucket in TFRecord format. You can specify the training, testing, and validation percentage splits. By default, the split is 1 or 100% for the training set and 0 or 0% for testing and validation sets. When setting the dataset split, the sum of training, testing, and validation needs to add up to 1 or 100% (for example, 0.6+0.2+0.2). Dataflow automatically determines the optimal number of shards for each output dataset.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/Cloud_BigQuery_to_GCS_TensorFlow_Records",
  "type": "CLASSIC",
  "params": {
    "required": [
      "readQuery",
      "outputDirectory"
    ],
    "optional": [
      "readIdColumn",
      "invalidOutputPath",
      "outputSuffix",
      "trainingPercentage",
      "testingPercentage",
      "validationPercentage"
    ]
  }
},
{
  "template_name": "Cassandra_To_Cloud_Bigtable",
  "description": "The Apache Cassandra to Cloud Bigtable template copies a table from Apache Cassandra to Cloud Bigtable. This template requires minimal configuration and replicates the table structure in Cassandra as closely as possible in Cloud Bigtable.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/Cassandra_To_Cloud_Bigtable",
  "type": "CLASSIC",
  "params": {
    "required": [
      "cassandraHosts",
      "cassandraKeyspace",
      "cassandraTable",
      "bigtableProjectId",
      "bigtableInstanceId",
      "bigtableTableId"
    ],
    "optional": [
      "cassandraPort",
      "defaultColumnFamily",
      "rowKeySeparator",
      "splitLargeRows",
      "writetimeCassandraColumnSchema",
      "setZeroTimestamp"
    ]
  }
},
{
  "template_name": "Cloud_Bigtable_to_GCS_Avro",
  "description": "The Bigtable to Cloud Storage Avro template is a pipeline that reads data from a Bigtable table and writes it to a Cloud Storage bucket in Avro format. You can use the template to move data from Bigtable to Cloud Storage.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/Cloud_Bigtable_to_GCS_Avro",
  "type": "CLASSIC",
  "params": {
    "required": [
      "bigtableProjectId",
      "bigtableInstanceId",
      "bigtableTableId",
      "outputDirectory",
      "filenamePrefix"
    ],
    "optional": [
      "bigtableAppProfileId"
    ]
  }
},
{
  "template_name": "Cloud_Bigtable_to_GCS_Json",
  "description": "The Bigtable to JSON template is a pipeline that reads data from a Bigtable table and writes it to a Cloud Storage bucket in JSON format.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/Cloud_Bigtable_to_GCS_Json",
  "type": "CLASSIC",
  "params": {
    "required": [
      "bigtableProjectId",
      "bigtableInstanceId",
      "bigtableTableId",
      "outputDirectory"
    ],
    "optional": [
      "filenamePrefix",
      "userOption",
      "columnsAliases",
      "bigtableAppProfileId"
    ]
  }
},
{
  "template_name": "GCS_CSV_to_BigQuery",
  "description": "The Cloud Storage CSV to BigQuery pipeline is a batch pipeline that allows you to read CSV files stored in Cloud Storage, and append the result to a BigQuery table.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/GCS_CSV_to_BigQuery",
  "type": "CLASSIC",
  "params": {
    "required": [
      "inputFilePattern",
      "schemaJSONPath",
      "outputTable",
      "bigQueryLoadingTemporaryDirectory",
      "badRecordsOutputTable",
      "delimiter",
      "csvFormat"
    ],
    "optional": [
      "containsHeaders",
      "csvFileEncoding"
    ]
  }
},
{
  "template_name": "Cloud_Bigtable_to_GCS_Parquet",
  "description": "The Bigtable to Cloud Storage Parquet template is a pipeline that reads data from a Bigtable table and writes it to a Cloud Storage bucket in Parquet format. You can use the template to move data from Bigtable to Cloud Storage.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/Cloud_Bigtable_to_GCS_Parquet",
  "type": "CLASSIC",
  "params": {
    "required": [
      "bigtableProjectId",
      "bigtableInstanceId",
      "bigtableTableId",
      "outputDirectory",
      "filenamePrefix"
    ],
    "optional": [
      "numShards",
      "bigtableAppProfileId"
    ]
  }
},
{
  "template_name": "Cloud_Bigtable_to_GCS_SequenceFile",
  "description": "The Bigtable to Cloud Storage SequenceFile template is a pipeline that reads data from a Bigtable table and writes the data to a Cloud Storage bucket in SequenceFile format. You can use the template to copy data from Bigtable to Cloud Storage.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/Cloud_Bigtable_to_GCS_SequenceFile",
  "type": "CLASSIC",
  "params": {
    "required": [
      "bigtableProject",
      "bigtableInstanceId",
      "bigtableTableId",
      "destinationPath",
      "filenamePrefix"
    ],
    "optional": [
      "bigtableAppProfileId",
      "bigtableStartRow",
      "bigtableStopRow",
      "bigtableMaxVersions",
      "bigtableFilter"
    ]
  }
},
{
  "template_name": "Cloud_Bigtable_to_Vector_Embeddings",
  "description": "The Bigtable to Vector Embedding template is a pipeline that reads data from a Bigtable table and writes it to a Cloud Storage bucket in JSON format, for vector embeddings.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/Cloud_Bigtable_to_Vector_Embeddings",
  "type": "CLASSIC",
  "params": {
    "required": [
      "bigtableProjectId",
      "bigtableInstanceId",
      "bigtableTableId",
      "outputDirectory",
      "idColumn",
      "embeddingColumn"
    ],
    "optional": [
      "filenamePrefix",
      "crowdingTagColumn",
      "embeddingByteSize",
      "allowRestrictsMappings",
      "denyRestrictsMappings",
      "intNumericRestrictsMappings",
      "floatNumericRestrictsMappings",
      "doubleNumericRestrictsMappings",
      "bigtableAppProfileId"
    ]
  }
},
{
  "template_name": "Cloud_Spanner_to_GCS_Avro",
  "description": "The Cloud Spanner to Avro Files on Cloud Storage template is a batch pipeline that exports a whole Cloud Spanner database to Cloud Storage in Avro format.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/Cloud_Spanner_to_GCS_Avro",
  "type": "CLASSIC",
  "params": {
    "required": [
      "instanceId",
      "databaseId",
      "outputDir"
    ],
    "optional": [
      "avroTempDirectory",
      "spannerHost",
      "snapshotTime",
      "spannerProjectId",
      "shouldExportTimestampAsLogicalType",
      "tableNames",
      "shouldExportRelatedTables",
      "spannerPriority",
      "dataBoostEnabled"
    ]
  }
},
{
  "template_name": "Spanner_to_GCS_Text",
  "description": "The Cloud Spanner to Cloud Storage Text template is a batch pipeline that reads in data from a Cloud Spanner table, and writes it to Cloud Storage as CSV text files.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/Spanner_to_GCS_Text",
  "type": "CLASSIC",
  "params": {
    "required": [
      "spannerTable",
      "spannerProjectId",
      "spannerInstanceId",
      "spannerDatabaseId",
      "textWritePrefix"
    ],
    "optional": [
      "csvTempDirectory",
      "spannerPriority",
      "spannerHost",
      "spannerSnapshotTime",
      "dataBoostEnabled"
    ]
  }
},
{
  "template_name": "Cloud_Spanner_vectors_to_Cloud_Storage",
  "description": "The Cloud Spanner to Vector Embeddings on Cloud Storage template is a batch pipeline that exports vector embeddings data from Cloud Spanner's table to Cloud Storage in JSON format. Vector embeddings are exported to a Cloud Storage folder specified by the user in the template parameters. The Cloud Storage folder will contain the list of exported .json files representing vector embeddings in a format supported by Vertex AI Vector Search Index.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/Cloud_Spanner_vectors_to_Cloud_Storage",
  "type": "CLASSIC",
  "params": {
    "required": [
      "spannerProjectId",
      "spannerInstanceId",
      "spannerDatabaseId",
      "spannerTable",
      "spannerColumnsToExport",
      "gcsOutputFolder",
      "gcsOutputFilePrefix"
    ],
    "optional": [
      "spannerHost",
      "spannerVersionTime",
      "spannerDataBoostEnabled",
      "spannerPriority"
    ]
  }
},
{
  "template_name": "GCS_Text_to_Cloud_PubSub",
  "description": "This template creates a batch pipeline that reads records from text files stored in Cloud Storage and publishes them to a Pub/Sub topic. The template can be used to publish records in a newline-delimited file containing JSON records or CSV file to a Pub/Sub topic for real-time processing. You can use this template to replay data to Pub/Sub.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/GCS_Text_to_Cloud_PubSub",
  "type": "CLASSIC",
  "params": {
    "required": [
      "inputFilePattern",
      "outputTopic"
    ],
    "optional": []
  }
},
{
  "template_name": "GCS_to_Elasticsearch",
  "description": "The Cloud Storage to Elasticsearch template is a batch pipeline that reads data from CSV files stored in a Cloud Storage bucket and writes the data into Elasticsearch as JSON documents.",
  "template_path": "v2/googlecloud-to-elasticsearch",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/GCS_to_Elasticsearch",
  "type": "FLEX",
  "params": {
    "required": [
      "deadletterTable",
      "inputFileSpec",
      "connectionUrl",
      "apiKey",
      "index"
    ],
    "optional": [
      "inputFormat",
      "containsHeaders",
      "delimiter",
      "csvFormat",
      "jsonSchemaPath",
      "largeNumFiles",
      "csvFileEncoding",
      "logDetailedCsvConversionErrors",
      "elasticsearchUsername",
      "elasticsearchPassword",
      "batchSize",
      "batchSizeBytes",
      "maxRetryAttempts",
      "maxRetryDuration",
      "propertyAsIndex",
      "javaScriptIndexFnGcsPath",
      "javaScriptIndexFnName",
      "propertyAsId",
      "javaScriptIdFnGcsPath",
      "javaScriptIdFnName",
      "javaScriptTypeFnGcsPath",
      "javaScriptTypeFnName",
      "javaScriptIsDeleteFnGcsPath",
      "javaScriptIsDeleteFnName",
      "usePartialUpdate",
      "bulkInsertMethod",
      "trustSelfSignedCerts",
      "disableCertificateValidation",
      "apiKeyKMSEncryptionKey",
      "apiKeySecretId",
      "apiKeySource",
      "socketTimeout",
      "javascriptTextTransformGcsPath",
      "javascriptTextTransformFunctionName"
    ]
  }
},
{
  "template_name": "GCS_to_Elasticsearch_Xlang",
  "description": "The Cloud Storage to Elasticsearch template is a batch pipeline that reads data from CSV files stored in a Cloud Storage bucket and writes the data into Elasticsearch as JSON documents. Alternatively, you can provide a Python user-defined function (UDF) that parses the CSV text and outputs Elasticsearch documents.",
  "template_path": "v2/googlecloud-to-elasticsearch",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/GCS_to_Elasticsearch_Xlang",
  "type": "FLEX",
  "params": {
    "required": [
      "deadletterTable",
      "inputFileSpec",
      "connectionUrl",
      "apiKey",
      "index"
    ],
    "optional": [
      "inputFormat",
      "containsHeaders",
      "delimiter",
      "csvFormat",
      "jsonSchemaPath",
      "largeNumFiles",
      "csvFileEncoding",
      "logDetailedCsvConversionErrors",
      "elasticsearchUsername",
      "elasticsearchPassword",
      "batchSize",
      "batchSizeBytes",
      "maxRetryAttempts",
      "maxRetryDuration",
      "propertyAsIndex",
      "javaScriptIndexFnGcsPath",
      "javaScriptIndexFnName",
      "propertyAsId",
      "javaScriptIdFnGcsPath",
      "javaScriptIdFnName",
      "javaScriptTypeFnGcsPath",
      "javaScriptTypeFnName",
      "javaScriptIsDeleteFnGcsPath",
      "javaScriptIsDeleteFnName",
      "usePartialUpdate",
      "bulkInsertMethod",
      "trustSelfSignedCerts",
      "disableCertificateValidation",
      "apiKeyKMSEncryptionKey",
      "apiKeySecretId",
      "apiKeySource",
      "socketTimeout",
      "pythonExternalTextTransformGcsPath",
      "pythonExternalTextTransformFunctionName"
    ]
  }
},
{
  "template_name": "Dataplex_JDBC_Ingestion",
  "description": "A pipeline that reads from a JDBC source and writes to to a Dataplex asset, which can be either a BigQuery dataset or a Cloud Storage bucket. JDBC connection string, user name and password can be passed in directly as plaintext or encrypted using the Google Cloud KMS API. If the parameter KMSEncryptionKey is specified, connectionURL, username, and password should be all in encrypted format.",
  "template_path": "v2/dataplex",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Dataplex_JDBC_Ingestion",
  "type": "FLEX",
  "params": {
    "required": [
      "connectionURL",
      "driverClassName",
      "driverJars",
      "query",
      "outputTable",
      "outputAsset"
    ],
    "optional": [
      "connectionProperties",
      "username",
      "password",
      "KMSEncryptionKey",
      "partitioningScheme",
      "paritionColumn",
      "writeDisposition",
      "fileFormat",
      "useColumnAlias",
      "fetchSize",
      "updateDataplexMetadata",
      "useStorageWriteApi",
      "useStorageWriteApiAtLeastOnce"
    ]
  }
},
{
  "template_name": "Dataplex_File_Format_Conversion",
  "description": "A pipeline that converts file format of Cloud Storage files, registering metadata for the newly created files in Dataplex.",
  "template_path": "v2/dataplex",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Dataplex_File_Format_Conversion",
  "type": "FLEX",
  "params": {
    "required": [
      "inputAssetOrEntitiesList",
      "outputFileFormat",
      "outputAsset"
    ],
    "optional": [
      "outputFileCompression",
      "writeDisposition",
      "updateDataplexMetadata"
    ]
  }
},
{
  "template_name": "Dataplex_BigQuery_to_GCS",
  "description": "A pipeline that exports all tables from a BigQuery dataset to Cloud Storage, registering metadata for the newly created files in Dataplex.",
  "template_path": "v2/dataplex",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Dataplex_BigQuery_to_GCS",
  "type": "FLEX",
  "params": {
    "required": [
      "sourceBigQueryDataset",
      "destinationStorageBucketAssetName",
      "maxParallelBigQueryMetadataRequests"
    ],
    "optional": [
      "tables",
      "exportDataModifiedBeforeDateTime",
      "fileFormat",
      "fileCompression",
      "partitionIdRegExp",
      "writeDisposition",
      "enforceSamePartitionKey",
      "deleteSourceData",
      "updateDataplexMetadata"
    ]
  }
},
{
  "template_name": "Firestore_to_GCS_Text",
  "description": "The Firestore to Cloud Storage Text template is a batch pipeline that reads Firestore entities and writes them to Cloud Storage as text files. You can provide a function to process each entity as a JSON string. If you don't provide such a function, every line in the output file will be a JSON-serialized entity.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/Firestore_to_GCS_Text",
  "type": "CLASSIC",
  "params": {
    "required": [
      "firestoreReadGqlQuery",
      "firestoreReadProjectId",
      "textWritePrefix"
    ],
    "optional": [
      "firestoreReadNamespace",
      "javascriptTextTransformGcsPath",
      "javascriptTextTransformFunctionName"
    ]
  }
},
{
  "template_name": "Google_Ads_to_BigQuery",
  "description": "The Google Ads to BigQuery template is a batch pipeline that reads Google Ads reports and writes to BigQuery.",
  "template_path": "v2/google-ads-to-googlecloud",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Google_Ads_to_BigQuery",
  "type": "FLEX",
  "params": {
    "required": [
      "customerIds",
      "query",
      "qpsPerWorker",
      "googleAdsClientId",
      "googleAdsClientSecret",
      "googleAdsRefreshToken",
      "googleAdsDeveloperToken",
      "outputTableSpec"
    ],
    "optional": [
      "loginCustomerId",
      "bigQueryTableSchemaPath",
      "writeDisposition",
      "createDisposition"
    ]
  }
},
{
  "template_name": "Google_Cloud_to_Neo4j",
  "description": "The Google Cloud to Neo4j template lets you import a dataset into a Neo4j database through a Dataflow job, sourcing data from CSV files hosted in Google Cloud Storage buckets. It also lets you to manipulate and transform the data at various steps of the import. You can use the template for both first-time imports and incremental imports.",
  "template_path": "v2/googlecloud-to-neo4j",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Google_Cloud_to_Neo4j",
  "type": "FLEX",
  "params": {
    "required": [
      "jobSpecUri"
    ],
    "optional": [
      "neo4jConnectionUri",
      "neo4jConnectionSecretId",
      "optionsJson",
      "readQuery",
      "inputFilePattern",
      "disabledAlgorithms",
      "extraFilesToStage"
    ]
  }
},
{
  "template_name": "Jdbc_to_BigQuery",
  "description": "The JDBC to BigQuery template is a batch pipeline that copies data from a relational database table into an existing BigQuery table. This pipeline uses JDBC to connect to the relational database. You can use this template to copy data from any relational database with available JDBC drivers into BigQuery. For an extra layer of protection, you can also pass in a Cloud KMS key along with a Base64-encoded username, password, and connection string parameters encrypted with the Cloud KMS key. See the Cloud KMS API encryption endpoint for additional details on encrypting your username, password, and connection string parameters.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/Jdbc_to_BigQuery",
  "type": "CLASSIC",
  "params": {
    "required": [
      "driverJars",
      "driverClassName",
      "connectionURL",
      "query",
      "outputTable",
      "bigQueryLoadingTemporaryDirectory"
       
    ],
    "optional": [
      "connectionProperties",
      "username",
      "password",
      "KMSEncryptionKey",
      "useColumnAlias",
      "disabledAlgorithms",
      "extraFilesToStage"
    ]
  }
},
{
  "template_name": "Jdbc_to_BigQuery_Flex",
  "description": "The JDBC to BigQuery template is a batch pipeline that copies data from a relational database table into an existing BigQuery table. This pipeline uses JDBC to connect to the relational database. You can use this template to copy data from any relational database with available JDBC drivers into BigQuery.",
  "template_path": "v2/jdbc-and-googlecloud",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Jdbc_to_BigQuery_Flex",
  "type": "FLEX",
  "params": {
    "required": [
      "driverJars",
      "driverClassName",
      "connectionURL",
      "outputTable",
      "bigQueryLoadingTemporaryDirectory"
    ],
    "optional": [
      "connectionProperties",
      "username",
      "password",
      "query",
      "KMSEncryptionKey",
      "useColumnAlias",
      "isTruncate",
      "partitionColumn",
      "partitionColumnType",
      "table",
      "numPartitions",
      "lowerBound",
      "upperBound",
      "fetchSize",
      "createDisposition",
      "bigQuerySchemaPath",
      "outputDeadletterTable",
      "disabledAlgorithms",
      "extraFilesToStage",
      "useStorageWriteApi",
      "useStorageWriteApiAtLeastOnce"
    ]
  }
},
{
  "template_name": "Jdbc_to_PubSub",
  "description": "The Java Database Connectivity (JDBC) to Pub/Sub template is a batch pipeline that ingests data from JDBC source and writes the resulting records to a pre-existing Pub/Sub topic as a JSON string.",
  "template_path": "v2/jdbc-and-googlecloud",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Jdbc_to_PubSub",
  "type": "FLEX",
  "params": {
    "required": [
      "driverClassName",
      "connectionUrl",
      "driverJars",
      "query",
      "outputTopic"
    ],
    "optional": [
      "username",
      "password",
      "connectionProperties",
      "KMSEncryptionKey",
      "disabledAlgorithms",
      "extraFilesToStage"
    ]
  }
},
{
  "template_name": "Managed_IO_to_Managed_IO",
  "description": "The Managed I/O to Managed I/O template is a flexible pipeline that can read from any Managed I/O source and write to any Managed I/O sink. This template supports all available Managed I/O connectors including ICEBERG, ICEBERG_CDC, KAFKA, and BIGQUERY. The template automatically determines whether to run in streaming or batch mode based on the source connector type (ICEBERG_CDC and KAFKA use streaming, others use batch).",
  "template_path": "v2/managed-io-to-managed-io",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Managed_IO_to_Managed_IO",
  "type": "FLEX",
  "params": {
    "required": [
      "sourceConnectorType",
      "sourceConfig",
      "sinkConnectorType",
      "sinkConfig"
    ],
    "optional": []
  }
},
{
  "template_name": "MySQL_to_BigQuery",
  "description": "The MySQL to BigQuery template is a batch pipeline that copies data from a MySQL table into an existing BigQuery table. This pipeline uses JDBC to connect to MySQL.",
  "template_path": "v2/mysql-to-googlecloud",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/MySQL_to_BigQuery",
  "type": "FLEX",
  "params": {
    "required": [
      "connectionURL",
      "outputTable",
      "bigQueryLoadingTemporaryDirectory"
    ],
    "optional": [
      "connectionProperties",
      "username",
      "password",
      "query",
      "KMSEncryptionKey",
      "useColumnAlias",
      "isTruncate",
      "partitionColumn",
      "partitionColumnType",
      "table",
      "numPartitions",
      "lowerBound",
      "upperBound",
      "fetchSize",
      "createDisposition",
      "bigQuerySchemaPath",
      "outputDeadletterTable",
      "disabledAlgorithms",
      "extraFilesToStage",
      "useStorageWriteApi",
      "useStorageWriteApiAtLeastOnce"
    ]
  }
},
{
  "template_name": "Oracle_to_BigQuery",
  "description": "The Oracle to BigQuery template is a batch pipeline that copies data from an Oracle table into an existing BigQuery table. This pipeline uses JDBC to connect to Oracle.",
  "template_path": "v2/oracle-to-googlecloud",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Oracle_to_BigQuery",
  "type": "FLEX",
  "params": {
    "required": [
      "connectionURL",
      "outputTable",
      "bigQueryLoadingTemporaryDirectory"
    ],
    "optional": [
      "connectionProperties",
      "username",
      "password",
      "query",
      "KMSEncryptionKey",
      "useColumnAlias",
      "isTruncate",
      "partitionColumn",
      "partitionColumnType",
      "table",
      "numPartitions",
      "lowerBound",
      "upperBound",
      "fetchSize",
      "createDisposition",
      "bigQuerySchemaPath",
      "outputDeadletterTable",
      "disabledAlgorithms",
      "extraFilesToStage",
      "useStorageWriteApi",
      "useStorageWriteApiAtLeastOnce"
    ]
  }
},
{
  "template_name": "GCS_Parquet_to_Cloud_Bigtable",
  "description": "The Cloud Storage Parquet to Bigtable template is a pipeline that reads data from Parquet files in a Cloud Storage bucket and writes the data to a Bigtable table. You can use the template to copy data from Cloud Storage to Bigtable.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/GCS_Parquet_to_Cloud_Bigtable",
  "type": "CLASSIC",
  "params": {
    "required": [
      "bigtableProjectId",
      "bigtableInstanceId",
      "bigtableTableId",
      "inputFilePattern"
    ],
    "optional": [
      "splitLargeRows"
    ]
  }
},
{
  "template_name": "SQLServer_to_BigQuery",
  "description": "The SQL Server to BigQuery template is a batch pipeline that copies data from a SQL Server table into an existing BigQuery table.",
  "template_path": "v2/sqlserver-to-googlecloud",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/SQLServer_to_BigQuery",
  "type": "FLEX",
  "params": {
    "required": [
      "connectionURL",
      "outputTable",
      "bigQueryLoadingTemporaryDirectory"
    ],
    "optional": [
      "connectionProperties",
      "username",
      "password",
      "query",
      "KMSEncryptionKey",
      "useColumnAlias",
      "isTruncate",
      "partitionColumn",
      "partitionColumnType",
      "table",
      "numPartitions",
      "lowerBound",
      "upperBound",
      "fetchSize",
      "createDisposition",
      "bigQuerySchemaPath",
      "outputDeadletterTable",
      "disabledAlgorithms",
      "extraFilesToStage",
      "useStorageWriteApi",
      "useStorageWriteApiAtLeastOnce"
    ]
  }
},
{
  "template_name": "GCS_SequenceFile_to_Cloud_Bigtable",
  "description": "The Cloud Storage SequenceFile to Bigtable template is a pipeline that reads data from SequenceFiles in a Cloud Storage bucket and writes the data to a Bigtable table. You can use the template to copy data from Cloud Storage to Bigtable.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/GCS_SequenceFile_to_Cloud_Bigtable",
  "type": "CLASSIC",
  "params": {
    "required": [
      "bigtableProject",
      "bigtableInstanceId",
      "bigtableTableId",
      "sourcePattern"
    ],
    "optional": [
      "bigtableAppProfileId",
      "mutationThrottleLatencyMs"
    ]
  }
},
{
  "template_name": "Sourcedb_to_Spanner_Flex",
  "description": "The SourceDB to Spanner template is a batch pipeline that copies data from a relational database into an existing Spanner database. This pipeline uses JDBC to connect to the relational database. You can use this template to copy data from any relational database with available JDBC drivers into Spanner. This currently only supports a limited set of types of MySQL.",
  "template_path": "v2/sourcedb-to-spanner",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Sourcedb_to_Spanner_Flex",
  "type": "FLEX",
  "params": {
    "required": [
      "instanceId",
      "databaseId",
      "projectId",
      "outputDirectory"
    ],
    "optional": [
      "sourceDbDialect",
      "jdbcDriverJars",
      "jdbcDriverClassName",
      "sourceConfigURL",
      "username",
      "password",
      "tables",
      "numPartitions",
      "fetchSize",
      "spannerHost",
      "maxConnections",
      "sessionFilePath",
      "transformationJarPath",
      "transformationClassName",
      "transformationCustomParameters",
      "namespace",
      "insertOnlyModeForSpannerMutations",
      "batchSizeForSpannerMutations",
      "spannerPriority",
      "tableOverrides",
      "columnOverrides",
      "schemaOverridesFilePath",
      "uniformizationStageCountHint",
      "astraDBToken",
      "astraDBDatabaseId",
      "astraDBKeySpace",
      "astraDBRegion",
      "failureInjectionParameter",
      "disabledAlgorithms",
      "extraFilesToStage"
    ]
  }
},
{
  "template_name": "Cloud_Spanner_to_BigQuery_Flex",
  "description": "The Spanner to BigQuery template is a batch pipeline that reads data from a Spanner table, and writes them to a BigQuery table.",
  "template_path": "v2/googlecloud-to-googlecloud",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Cloud_Spanner_to_BigQuery_Flex",
  "type": "FLEX",
  "params": {
    "required": [
      "spannerInstanceId",
      "spannerDatabaseId",
      "outputTableSpec"
    ],
    "optional": [
      "spannerProjectId",
      "spannerTableId",
      "spannerRpcPriority",
      "sqlQuery",
      "bigQuerySchemaPath",
      "writeDisposition",
      "createDisposition",
      "useStorageWriteApi",
      "useStorageWriteApiAtLeastOnce"
    ]
  }
},
{
  "template_name": "GCS_Text_to_BigQuery",
  "description": "The Cloud Storage Text to BigQuery pipeline is a batch pipeline that allows you to read text files stored in Cloud Storage, transform them using a JavaScript User Defined Function (UDF) that you provide, and append the result to a BigQuery table.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/GCS_Text_to_BigQuery",
  "type": "CLASSIC",
  "params": {
    "required": [
      "inputFilePattern",
      "JSONPath",
      "outputTable",
      "bigQueryLoadingTemporaryDirectory"
    ],
    "optional": [
      "javascriptTextTransformGcsPath",
      "javascriptTextTransformFunctionName"
    ]
  }
},
{
  "template_name": "GCS_Text_to_BigQuery_Xlang",
  "description": "The Cloud Storage Text to BigQuery pipeline is a batch pipeline that allows you to read text files stored in Cloud Storage, transform them using a Python User Defined Function (UDF) that you provide, and append the result to a BigQuery table.",
  "template_path": "v2/googlecloud-to-googlecloud",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/GCS_Text_to_BigQuery_Xlang",
  "type": "FLEX",
  "params": {
    "required": [
      "inputFilePattern",
      "JSONPath",
      "outputTable",
      "bigQueryLoadingTemporaryDirectory"
    ],
    "optional": [
      "useStorageWriteApi",
      "useStorageWriteApiAtLeastOnce",
      "pythonExternalTextTransformGcsPath",
      "pythonExternalTextTransformFunctionName"
    ]
  }
},
{
  "template_name": "GCS_Text_to_BigQuery_Flex",
  "description": "The Cloud Storage Text to BigQuery pipeline is a batch pipeline that allows you to read text files stored in Cloud Storage, transform them using a JavaScript User Defined Function (UDF) that you provide, and append the result to a BigQuery table.",
  "template_path": "v2/googlecloud-to-googlecloud",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/GCS_Text_to_BigQuery_Flex",
  "type": "FLEX",
  "params": {
    "required": [
      "inputFilePattern",
      "JSONPath",
      "outputTable",
      "javascriptTextTransformGcsPath",
      "javascriptTextTransformFunctionName",
      "bigQueryLoadingTemporaryDirectory"
    ],
    "optional": [
      "useStorageWriteApi",
      "useStorageWriteApiAtLeastOnce"
    ]
  }
},
{
  "template_name": "GCS_Text_to_Cloud_Spanner",
  "description": "The Cloud Storage Text to Cloud Spanner template is a batch pipeline that reads CSV text files from Cloud Storage and imports them to a Cloud Spanner database.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/GCS_Text_to_Cloud_Spanner",
  "type": "CLASSIC",
  "params": {
    "required": [
      "instanceId",
      "databaseId",
      "importManifest"
    ],
    "optional": [
      "spannerHost",
      "columnDelimiter",
      "fieldQualifier",
      "trailingDelimiter",
      "escape",
      "nullString",
      "dateFormat",
      "timestampFormat",
      "spannerProjectId",
      "spannerPriority",
      "handleNewLine",
      "invalidOutputPath"
    ]
  }
},
{
  "template_name": "GCS_Text_to_Firestore",
  "description": "The Cloud Storage Text to Firestore template is a batch pipeline that reads from text files stored in Cloud Storage and writes JSON encoded Entities to Firestore. Each line in the input text files must be in the specified JSON format.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/GCS_Text_to_Firestore",
  "type": "CLASSIC",
  "params": {
    "required": [
      "textReadPattern",
      "firestoreWriteProjectId",
      "errorWritePath"
    ],
    "optional": [
      "javascriptTextTransformGcsPath",
      "javascriptTextTransformFunctionName",
      "firestoreHintNumWorkers"
    ]
  }
},
{
  "template_name": "Bulk_Compress_GCS_Files",
  "description": "The Bulk Compress Cloud Storage Files template is a batch pipeline that compresses files on Cloud Storage to a specified location. This template can be useful when you need to compress large batches of files as part of a periodic archival process. The supported compression modes are: BZIP2, DEFLATE, GZIP.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/Bulk_Compress_GCS_Files",
  "type": "CLASSIC",
  "params": {
    "required": [
      "inputFilePattern",
      "outputDirectory",
      "outputFailureFile",
      "compression"
    ],
    "optional": [
      "outputFilenameSuffix"
    ]
  }
},
{
  "template_name": "Bulk_Decompress_GCS_Files",
  "description": "The Bulk Decompress Cloud Storage Files template is a batch pipeline that decompresses files on Cloud Storage to a specified location. This functionality is useful when you want to use compressed data to minimize network bandwidth costs during a migration, but would like to maximize analytical processing speed by operating on uncompressed data after migration. The pipeline automatically handles multiple compression modes during a single run and determines the decompression mode to use based on the file extension (.bzip2, .deflate, .gz, .zip).",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/Bulk_Decompress_GCS_Files",
  "type": "CLASSIC",
  "params": {
    "required": [
      "inputFilePattern",
      "outputDirectory",
      "outputFailureFile"
    ],
    "optional": []
  }
},
{
  "template_name": "Firestore_to_Firestore_Delete",
  "description": "A pipeline which reads in Entities (via a GQL query) from Firestore, optionally passes in the JSON encoded Entities to a JavaScript UDF, and then deletes all matching Entities in the selected target project.",
  "template_path": "v1/",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/Firestore_to_Firestore_Delete",
  "type": "CLASSIC",
  "params": {
    "required": [
      "firestoreReadGqlQuery",
      "firestoreReadProjectId",
      "firestoreDeleteProjectId"
    ],
    "optional": [
      "firestoreReadNamespace",
      "firestoreHintNumWorkers",
      "javascriptTextTransformGcsPath",
      "javascriptTextTransformFunctionName"
    ]
  }
},
{
  "template_name": "File_Format_Conversion",
  "description": "The File Format Conversion template is a batch pipeline that converts files stored on Cloud Storage from one supported format to another.",
  "template_path": "v2/file-format-conversion",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/File_Format_Conversion",
  "type": "FLEX",
  "params": {
    "required": [
      "inputFileFormat",
      "outputFileFormat",
      "inputFileSpec",
      "outputBucket",
      "schema"
    ],
    "optional": [
      "containsHeaders",
      "deadletterTable",
      "delimiter",
      "csvFormat",
      "jsonSchemaPath",
      "largeNumFiles",
      "csvFileEncoding",
      "logDetailedCsvConversionErrors",
      "numShards",
      "outputFilePrefix"
    ]
  }
},
{
  "template_name": "Streaming_Data_Generator",
  "description": "A pipeline to publish messages at specified QPS.This template can be used to benchmark performance of streaming pipelines.",
  "template_path": "v2/streaming-data-generator",
  "template_gcs_path": "gs://dataflow-templates-us-central1/latest/flex/Streaming_Data_Generator",
  "type": "FLEX",
  "params": {
    "required": [
      "qps"
    ],
    "optional": [
      "schemaTemplate",
      "schemaLocation",
      "topic",
      "messagesLimit",
      "outputType",
      "avroSchemaLocation",
      "sinkType",
      "outputTableSpec",
      "writeDisposition",
      "outputDeadletterTable",
      "windowDuration",
      "outputDirectory",
      "outputFilenamePrefix",
      "numShards",
      "driverClassName",
      "connectionUrl",
      "username",
      "password",
      "connectionProperties",
      "statement",
      "projectId",
      "spannerInstanceName",
      "spannerDatabaseName",
      "spannerTableName",
      "maxNumMutations",
      "maxNumRows",
      "batchSizeBytes",
      "commitDeadlineSeconds",
      "bootstrapServer",
      "kafkaTopic"
    ]
  }
}

]
