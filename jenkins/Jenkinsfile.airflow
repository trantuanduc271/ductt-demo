pipeline {
    agent {
        kubernetes {
            yaml '''
apiVersion: v1
kind: Pod
metadata:
  labels:
    jenkins: agent
spec:
  serviceAccountName: jenkins
  containers:
  - name: python
    image: python:3.11-slim
    command:
    - cat
    tty: true
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "2Gi"
        cpu: "1000m"
'''
        }
    }
    
    environment {
        DAGS_DIR = 'dags'
        AIRFLOW_URL = 'https://airflow.ducttdevops.com'
        AIRFLOW_DAG_ID = "${env.AIRFLOW_DAG_ID ?: 'sales_analytics_pipeline'}"
        AIRFLOW_USER = 'admin'
        AIRFLOW_PASS = 'admin'
    }
    
    stages {
        stage('Checkout') {
            steps {
                checkout scm
            }
        }
        
        stage('Test DAG') {
            steps {
                container('python')                 {
                    script {
                        echo "Testing DAG: ${AIRFLOW_DAG_ID}"
                        
                        sh """
                            set -e
                            
                            # Install dependencies with constraint file
                            pip install --upgrade pip --quiet
                            pip install apache-airflow==2.8.0 apache-airflow-providers-postgres pandas scikit-learn joblib azure-storage-blob --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.8.0/constraints-3.11.txt" --quiet 2>&1 | grep -v WARNING || true
                            
                            echo ""
                            echo "TEST 1: Check DAG file exists"
                            if [ ! -f "dags/sale_analytics.py" ]; then
                                echo "ERROR: DAG file not found"
                                exit 1
                            fi
                            echo "Found: dags/sale_analytics.py"
                            
                            echo ""
                            echo "TEST 2: Validate Python syntax"
                            python -m py_compile dags/sale_analytics.py
                            echo "Python syntax valid"
                            
                            echo ""
                            echo "TEST 3: Test DAG import"
                            
                            export AIRFLOW_HOME=/tmp/airflow
                            export AIRFLOW__CORE__LOAD_EXAMPLES=False
                            export AIRFLOW__CORE__UNIT_TEST_MODE=True
                            
                            python << 'PYTHON_SCRIPT'
import sys
import os
from pathlib import Path

# Verify airflow installation
try:
    import airflow
    print(f"Airflow {airflow.__version__} loaded")
except ImportError as e:
    print(f"ERROR: Cannot import airflow - {e}")
    sys.exit(1)

# Import Airflow classes
try:
    from airflow import DAG
    from airflow.models import DagBag
    print("Airflow DAG class imported")
except ImportError as e:
    print(f"ERROR: Cannot import DAG class - {e}")
    sys.exit(1)

# Set DAG file path
dag_file = "dags/sale_analytics.py"
expected_dag_id = "sales_analytics_pipeline"

try:
    # Use DagBag to load DAG
    print(f"Loading DAG from: {os.path.abspath(dag_file)}")
    
    # Create a DagBag with the dags folder
    dagbag = DagBag(dag_folder="dags", include_examples=False)
    
    if dagbag.import_errors:
        print("ERROR: DAG import failed")
        for filename, error in dagbag.import_errors.items():
            print(f"  {filename}: {error}")
        sys.exit(1)
    
    print(f"Module imported successfully")
    
    # Find DAG objects
    if not dagbag.dags:
        print("ERROR: No DAG objects found")
        sys.exit(1)
    
    found_dags = []
    for dag_id, dag in dagbag.dags.items():
        found_dags.append({
            "name": dag_id,
            "dag_id": dag.dag_id,
            "tasks": len(dag.tasks)
        })
    
    print(f"Found {len(found_dags)} DAG(s):")
    for dag_info in found_dags:
        print(f'  {dag_info["name"]} - {dag_info["tasks"]} tasks')
    
    # Check if requested DAG ID matches
    if expected_dag_id:
        if expected_dag_id in dagbag.dags:
            print(f"Found target DAG: {expected_dag_id}")
            dag = dagbag.dags[expected_dag_id]
            print(f"  Tasks: {', '.join([t.task_id for t in dag.tasks])}")
        else:
            print(f"ERROR: DAG {expected_dag_id} not found")
            print(f"Available: {list(dagbag.dags.keys())}")
            sys.exit(1)
    
    # Test 4: Validate DAG structure
    print("")
    print("TEST 4: Validate DAG structure")
    
    with open(dag_file, "r") as f:
        content = f.read()
    
    checks = {
        "has_dag_import": "from airflow import DAG" in content or "import airflow" in content,
        "has_dag_definition": "DAG(" in content or "with DAG(" in content,
        "has_tasks": "Operator(" in content or "PythonOperator" in content or "PostgresOperator" in content,
    }
    
    all_passed = True
    for check_name, passed in checks.items():
        check_display = check_name.replace("_", " ").replace("has ", "")
        if passed:
            print(f"  {check_display}")
        else:
            print(f"  MISSING: {check_display}")
            all_passed = False
    
    if not all_passed:
        print("ERROR: DAG structure validation failed")
        sys.exit(1)
    
    print("")
    print("Validation complete")
    
except Exception as e:
    print(f"ERROR: DAG validation failed - {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
PYTHON_SCRIPT
                        """
                    }
                }
            }
        }
        
        stage('Run Airflow DAG') {
            when {
                expression { return currentBuild.result == null || currentBuild.result == 'SUCCESS' }
            }
            steps {
                container('python')                 {
                    script {
                        echo "Triggering DAG: ${AIRFLOW_DAG_ID}"
                        
                        sh """
                            pip install requests --quiet 2>&1 | grep -v WARNING || true
                            
                            python << 'PYTHON_SCRIPT'
import requests
import json
import base64
import sys
import os
import urllib3

# Disable SSL warnings for self-signed certificates
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

airflow_url = '${AIRFLOW_URL}'
dag_id = '${AIRFLOW_DAG_ID}'
username = '${AIRFLOW_USER}'
password = '${AIRFLOW_PASS}'

# Create auth header
auth_string = f'{username}:{password}'
auth_bytes = auth_string.encode('ascii')
auth_b64 = base64.b64encode(auth_bytes).decode('ascii')

# Trigger DAG
trigger_url = f'{airflow_url}/api/v1/dags/{dag_id}/dagRuns'
dag_run_id = f'jenkins_{os.environ.get("BUILD_NUMBER", "manual")}_{int(__import__("time").time())}'

payload = {
    'dag_run_id': dag_run_id,
    'conf': {
        'triggered_by': 'jenkins',
        'build_number': os.environ.get('BUILD_NUMBER', 'unknown'),
        'job_name': os.environ.get('JOB_NAME', 'unknown')
    }
}

headers = {
    'Authorization': f'Basic {auth_b64}',
    'Content-Type': 'application/json'
}

try:
    print(f'Triggering DAG: {dag_id}')
    print(f'API URL: {trigger_url}')
    
    response = requests.post(
        trigger_url,
        headers=headers,
        json=payload,
        timeout=30,
        verify=False  # Disable SSL verification for self-signed certificates
    )
    
    response.raise_for_status()
    result = response.json()
    
    print(f'Triggered successfully')
    print(f'Run ID: {result.get("dag_run_id", dag_run_id)}')
    print(f'State: {result.get("state", "unknown")}')
    print(f'View at: {airflow_url}/dags/{dag_id}/grid?dag_run_id={result.get("dag_run_id", dag_run_id)}')
    
except requests.exceptions.RequestException as e:
    print(f'ERROR: Failed to trigger DAG - {e}')
    if hasattr(e, 'response') and e.response is not None:
        print(f'Response: {e.response.text}')
    sys.exit(1)
PYTHON_SCRIPT
                        """
                    }
                }
            }
        }
    }
    
    post {
        success {
            echo "DAG validation and trigger completed"
        }
        failure {
            echo "DAG validation or trigger failed"
        }
    }
}